{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to build a very elementary neural network, trying to predict the expression of certain genes based on DNA sequences in non-coding regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "We first perform the **one hot encoding** to translate the DNA based \"AGCT\" into corresponding 0/1 values. One thing to note is that there does exist 'n's in lots of DNA sequences, and we treat them as all false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pairs = {'A': [1, 0, 0, 0], \n",
    "'C': [0, 1, 0, 0],\n",
    "'G': [0, 0, 1, 0],\n",
    "'T': [0, 0, 0, 1],\n",
    "'a': [1, 0, 0, 0],\n",
    "'c': [0, 1, 0, 0],\n",
    "'g': [0, 0, 1, 0],\n",
    "'t': [0, 0, 0, 1],\n",
    "'n': [0, 0, 0, 0],\n",
    "'N': [0, 0, 0, 0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are some functions to get the one hot encoded DNA data into some input matrix that can be fed into neural network algorithms. The major things to note are the following:\n",
    "1. DNA sequences are of difference lengths, some very short (100~ bases), some very long (3000~ bases). Since most sequences are in the length range 1000 - 2000, we decide to only take the first 1000 bases of each sequence to train the neural network and make the predictions. If too long, simply truncate it to length 1000. If too short, simply fill with zeros to extend it. \n",
    "2. DNA sequences are in different strands, some in negative strand, some in positive. We take the complement of the sequence if it is taken form the negative strand so thsat all our data is from the same (positive) strand.\n",
    "3. The entire sequence is *flattend*. For example, AGCT would be transformed into [1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1] where the first four represent A and the next four represent G and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_sequence(length, sequence):\n",
    "    if len(sequence) > length:\n",
    "        aligned_seq = sequence[:length]\n",
    "    else:\n",
    "        aligned_seq = sequence + [0]*(length-len(sequence))\n",
    "    return aligned_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT used\n",
    "def to_positive_strand(strand, sequence):\n",
    "    if strand == '-':\n",
    "        unflattened_seq = [base_pairs[n] for n in sequence.complement()]\n",
    "    else:\n",
    "        unflattened_seq = [base_pairs[n] for n in sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_seq_record(seq_record, X, y, length_read):\n",
    "    header = seq_record.description.split('|')\n",
    "    expressed = int(header[1])\n",
    "    y.append(expressed)\n",
    "    # unflattened_seq = to_positive_strand(header[3], seq_record.seq)\n",
    "    # NO NEED to reverse complement\n",
    "    unflattened_seq = [base_pairs[n] for n in seq_record.seq]\n",
    "    flattened_seq = [i for x in unflattened_seq for i in x]\n",
    "    aligned_seq = align_sequence(length_read, flattened_seq)\n",
    "    X.append(np.array(aligned_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file, X, y, length_read):\n",
    "    seq_record_list = list(SeqIO.parse(\"../data/input/3.24_species_only/\" + file,\"fasta\"))\n",
    "    for i in range(len(seq_record_list)):\n",
    "        process_seq_record(seq_record_list[i], X, y, length_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training size is the number of files to read for training. Read 200 files would give us 2400 sequences. <br/> For this simple model, we use 4800 sequence to train the neural network and 480 sequences to test its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(training_size, test_size, length_read):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    file_count = 0\n",
    "    for file in os.listdir(\"../data/input/3.24_species_only\"):\n",
    "        if file.endswith(\".fa\"):\n",
    "            if (file_count < training_size):\n",
    "                read_file(file, X_train, y_train, length_read)\n",
    "            elif (file_count < training_size + test_size):\n",
    "                read_file(file, X_test, y_test, length_read)\n",
    "            file_count += 1\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def to_np_array(X_train, y_train, X_test, y_test):\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    if len(y_train.shape) == 1:\n",
    "        y_train = np.transpose(np.array([y_train]))\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.transpose(np.array(y_test))\n",
    "    if len(y_test.shape) == 1:\n",
    "        y_test = np.transpose(np.array([y_test]))\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We examine the shape of all the training and test data matrix to check that the above code works as we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4800, 4000), (4800, 1), (480, 4000), (480, 1)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = prepare_input(200, 20, 4000)\n",
    "X_train, y_train, X_test, y_test = to_np_array(X_train, y_train, X_test, y_test)\n",
    "[X_train.shape, y_train.shape, X_test.shape, y_test.shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Before actually getting into the neural network, we first try to implement a very simple logistic regression model to get a taste of the prediction procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model as lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lm.LogisticRegression()\n",
    "model.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.473"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = np.array(model.predict(X_test))\n",
    "round(sum(y_test.ravel() == y_predicted)/y_test.shape[0], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result of the rate of  correct prediction is slightly better, if any, than random guessing. This suggests that a lot of work needs to be done before we get a satisfying neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network with Keras\n",
    "Now that we have the data ready in the desired numpy array format with correct shapes, we can proceed to train the neural network with our training data in keras and use our test data to see how accurate it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell implements a sequential neural network with 1 input layer (4000 neurons), 4 hidden layers (1000, 400, 40, 10 neurons repectively) and 1 output layer with keras. <br/>\n",
    "All layers except the final use **relu** as activation functions while the final layer uses **sigmoid** as activation function. The cost fucntion is just defined as the binary crossentrophy. <br/> We train our neural network with 12000 DNA sequences in our training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "def train_nn(X_train, y_train, pr):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=1000, activation='relu', input_dim=4000))\n",
    "    model.add(Dense(units=400, activation='relu'))\n",
    "    model.add(Dense(units=40, activation='relu'))\n",
    "    model.add(Dense(units=10, activation='elu'))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(optimizer = 'SGD',\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=20, verbose = pr)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the accuracy on the training set reaches 83% after 10 epoches. <br/>\n",
    "Now we test the performance of this model with our test data. <br/> The test data contains 1200 DNA sequences. This size, I think, is already big enough for us to believe that the performance on this test data is quite representative of how the model would perform on new data in general (in other words, with little bias). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, X_test, y_test):\n",
    "    result = model.predict(X_test)\n",
    "    correct = list(np.apply_along_axis(lambda x: 0 if x<0.5 else 1, 1, result))==y_test.ravel()\n",
    "    return round(sum(correct)/y_test.shape[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4800/4800 [==============================] - 4s 759us/step - loss: 0.6819 - acc: 0.5719\n",
      "Epoch 2/20\n",
      "4800/4800 [==============================] - 3s 610us/step - loss: 0.6483 - acc: 0.6210\n",
      "Epoch 3/20\n",
      "4800/4800 [==============================] - 3s 612us/step - loss: 0.6156 - acc: 0.6740\n",
      "Epoch 4/20\n",
      "4800/4800 [==============================] - 3s 612us/step - loss: 0.5745 - acc: 0.7158\n",
      "Epoch 5/20\n",
      "4800/4800 [==============================] - 3s 624us/step - loss: 0.5257 - acc: 0.7654\n",
      "Epoch 6/20\n",
      "4800/4800 [==============================] - 3s 619us/step - loss: 0.4713 - acc: 0.8067\n",
      "Epoch 7/20\n",
      "4800/4800 [==============================] - 3s 621us/step - loss: 0.4128 - acc: 0.8417\n",
      "Epoch 8/20\n",
      "4800/4800 [==============================] - 3s 618us/step - loss: 0.3518 - acc: 0.8777\n",
      "Epoch 9/20\n",
      "4800/4800 [==============================] - 3s 620us/step - loss: 0.3185 - acc: 0.8802\n",
      "Epoch 10/20\n",
      "4800/4800 [==============================] - 3s 628us/step - loss: 0.2485 - acc: 0.9196\n",
      "Epoch 11/20\n",
      "4800/4800 [==============================] - 3s 620us/step - loss: 0.2149 - acc: 0.9302\n",
      "Epoch 12/20\n",
      "4800/4800 [==============================] - 3s 699us/step - loss: 0.2035 - acc: 0.9390\n",
      "Epoch 13/20\n",
      "4800/4800 [==============================] - 3s 644us/step - loss: 0.2122 - acc: 0.9200\n",
      "Epoch 14/20\n",
      "4800/4800 [==============================] - 3s 631us/step - loss: 0.1005 - acc: 0.9827\n",
      "Epoch 15/20\n",
      "4800/4800 [==============================] - 3s 637us/step - loss: 0.0739 - acc: 0.9915\n",
      "Epoch 16/20\n",
      "4800/4800 [==============================] - 3s 637us/step - loss: 0.0535 - acc: 0.9963\n",
      "Epoch 17/20\n",
      "4800/4800 [==============================] - 3s 717us/step - loss: 0.0403 - acc: 0.9973\n",
      "Epoch 18/20\n",
      "4800/4800 [==============================] - 3s 620us/step - loss: 0.0315 - acc: 0.9985\n",
      "Epoch 19/20\n",
      "4800/4800 [==============================] - 3s 630us/step - loss: 0.0255 - acc: 0.9994\n",
      "Epoch 20/20\n",
      "4800/4800 [==============================] - 3s 621us/step - loss: 0.0207 - acc: 0.9994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(train_nn(X_train, y_train, 1), X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is roughly 59%, slightly better than random guessing, which at least suggests that the neural network is actually running. Possibly also consider the ELU (exponential linear unit) activation function (shown here), although this may lead to overfittting. <br/>\n",
    "However, it is still far from what would be considered a satisfying prediction algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some major points to consider that may be helpful to improving the performance of the neural network:\n",
    "1. Use more complex neural network structure rather than the simple sequential model used above.\n",
    "2. Incorporate other information like what region of genome is the sequence located, the mapping of transcription factors, etc.\n",
    "3. Improve the way sequences of different lengths are aligned (Our current approach is truncating the long, filling zero with the short, which probably is too naive and causes significant loss of information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time estimate with data size increase\n",
    "\n",
    "In the previous work, we only used 500 files as training data and 50 files as test data. Also, only 1000 bases are read as all information after that is simply forgone. <br/>\n",
    "Now we want to see how much time approximately would it take for the program to run if we increase the sample size and the number of files we read in to train the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by studying the time required for reading in the file and turing the AGCT information into the desired matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_num = list(range(100, 3000, 100))\n",
    "time_taken = []\n",
    "\n",
    "for training_size in range(100, 1000, 50):\n",
    "    test_size = training_size * 0.1\n",
    "    start_time = timeit.default_timer()\n",
    "    X_train, y_train, X_test, y_test = prepare_input(training_size, test_size, 4000)\n",
    "    X_train, y_train, X_test, y_test = to_np_array(X_train, y_train, X_test, y_test)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(\"Time to read \" + str(int(training_size * 1.1)) + \" files is \" + str(round(elapsed, 2)) + \" seconds\")\n",
    "    time_taken.append(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to me that the time increase is approximately linear (O(n)) with the increase of the number of files to read in. Reading 3300 files (3000 for training and 300 for test) should take less than 3 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we study the time increase to train the neural network with the more data and also the improvement of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_num = list(range(100, 3000, 100))\n",
    "total_time_taken = []\n",
    "\n",
    "for training_size in range(100, 1000, 50):\n",
    "    test_size = training_size * 0.1\n",
    "    start_time = timeit.default_timer()\n",
    "    X_train, y_train, X_test, y_test = prepare_input(training_size, test_size, 4000)\n",
    "    X_train, y_train, X_test, y_test = to_np_array(X_train, y_train, X_test, y_test)\n",
    "    result = test_accuracy(train_nn(X_train, y_train, 0), X_test, y_test)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(\"Time to read and train with \" + str(int(training_size * 1.1)) + \" files is \" + str(round(elapsed, 2)) + \" seconds\"\n",
    "         + \", the result accuracy is \" + str(result))\n",
    "    total_time_taken.append(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(range(100, 1000, 100)), total_time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the time to read and train the neural network is also linear to the number of files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
